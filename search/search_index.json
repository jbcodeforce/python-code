{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python Studies","text":"<p>This repository regroup Python codes and notes from my own self-training and studies for web app development, crawling, Data AI, and even with raspberry PI work.</p>"},{"location":"#language-advantages-disadvantages","title":"Language Advantages / disadvantages","text":"<p>Advantages:</p> <ul> <li>Since 2022 Python is maybe the 1<sup>st</sup> most used programming language</li> <li>A lot of libraries, used by a lot of data scientists, data pipeline and web dev.</li> <li>Combines functional and Object Oriented Programming, with support to dynamic class creation and dynamic function call.</li> <li>Language of choices for Machine Learning development</li> <li>Raspberry PI language of choice</li> <li>Even new libraries are done to implement server side user interface, with project like Streamlit, gradio.app, Nice gui; taipy</li> </ul> <p>Disadvantages:</p> <ul> <li>Challenge in type enforcement and validation. Some tools are needed in IDE and adopting library like Pydantic.</li> <li>Not great for mobile and 3D game programming</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>See this good tutorial from Programiz.</p> <p>Python is an interpreted Object Oriented &amp; functional language. It organizes the code in modules. Use blank to indent code block. The coding style is known as PEP8.</p> <p>3.12 Release Product documentation</p> <p>For dev environment setting see the coding note.</p>"},{"location":"#vscode-and-python-extension","title":"VSCode and python extension","text":"<p>See dedicated getting started summary</p>"},{"location":"#python-readings","title":"Python readings","text":"<ul> <li>3.8 release Product documentation</li> <li>Python software foundation tutorial</li> <li>Tutorial from Programiz</li> <li>Using Docker For Python Web Development</li> <li>The Flask Mega-Tutorial</li> <li>Improve environment for Flask webapp</li> <li>10 Steps to Set Up Your Python Project for Success</li> <li>Kafka Python</li> <li>Python 2.0 Quick Reference: old web site</li> <li>Getting started with pymongo a mongodb driver</li> <li>pymongo documentation</li> </ul> <p>Statistics</p> <ul> <li>The Statistics and Calculus with Python Workshop</li> </ul>"},{"location":"astronomy/","title":"Python programming for astronomy","text":"Updates <p>Created 2019 Update 10/2025</p>"},{"location":"astronomy/#pulsars","title":"Pulsars","text":"<p>In 1967, a Cambridge University Ph.D. Student named Jocelyn Bell found something strange in her radio Astronomy Data: a regular pulse.</p> <p>When an old large star runs out of nuclear fuel, it rapidly collapses under its own gravity. The star's core suddenly transforms into a super dense ball of neutrons, and the outer layers of the star bounce off in a massive explosion of light and energy, a supernova. </p> <p>The law of conservation of angular momentum says, that when a star collapses the rotation speeds up. So, a neutron star could be spinning anything from once a second to tens or even hundreds of times a second. Also stars have magnetic fields, neutron stars would have incredibly intense magnetic fields. Charged particles in the super-hot plasma surrounding a neutron star, would get funneled towards the stars magnetic poles and shot out into space as two intense beams. Combining with rapid rotation, it becomes a flashing beams across the universe: A Pulsar</p> <p>The goal is to be able to answer: </p> <p>how many pulsars are detected in images taken with the Murchison Widefield Array telescope? </p> <p>The array telescope, detects radio emission at frequencies between 80 and 300 megahertz. It has a very large field of view, which means it's great for doing large survey projects.</p> <p> Images has using a grayscale to measure the flux density of emission from astronomical objects. Black is high flux density and gray is the background noise. </p> <p>Radio frequencies don't have color. These color maps are just used to accentuate different aspects of the intensity scale.</p> <p>In radio astronomy, flux density is measured in units of Janskys, which is equivalent to 10 to the power of -26 watts per square meter per hertz.</p> <p>)</p> <p>In other words, flux density is a measure of the spectral power received by a telescope detector of unit projected area.</p> <p>Astronomy images are usually stored in a file format called FITS, and to view them you can download software like DS9 or use an online tool like Aladin.</p> <p>We typically call something a detection, if the flux density is more than five standard deviations higher than the noise in the local region.</p> <p>To search from non-detection, a special approach is used called Stacking which  measures the statistical properties of a population we can't detect. Stacking works because the noise in a radio image is roughly random, with a Gaussian distribution centered on zero. When you add regions of an image that just have noise, the random numbers cancel out. But when you add regions of an image in which there are signals, the signals add together, increasing what we call the signal to noise ratio.</p> <p>Introduction to Pulsars (from CSIRO) Hobbs, M. (n.d.) from http://www.atnf.csiro.au/outreach/education/everyone/pulsars/index.html</p> <p>Pulsar Properties (from NRAO, advanced) National Radio Astronomy Observatory. (2010) from http://www.cv.nrao.edu/course/astr534/Pulsars.html</p>"},{"location":"astronomy/#calculating-the-mean-median-stack-of-a-set-of-fits-images","title":"Calculating the mean / median stack of a set of FITS images","text":"<p>In Flexible Image Transport System (FITS) the image is stored in a numerical array, which we can load into a NumPy array. Opening a FITS file in <code>astropy</code> returns a HDU (Header/Data Unit) list. Each HDU stores headers and (optionally) image data. Here is a program to find the point in the image with the maximum intensity:</p> <pre><code>from astropy.io import fits\ndef search_brightest_pixel(fname):\n  hdulist = fits.open(fname)\n  data = hdulist[0].data\n  nb_row,nb_col = data.shape\n  max = 0\n  x , y = (0,0)\n  for r in range(0,nb_row):\n    for c in range(0,nb_col):\n      if data[r][c] &gt; max :\n          x = r\n          y = c\n          max = data[r][c]\n  return x,y\n</code></pre> <p>The problem is that we load all the image in memory.</p> <p>A better approach is to use the median (the middle of the sorted data set), as the mean is easily skewed by outliers. (See program: astronomy/plot_mean_mediam.py)</p> <p>But getting median could get computational intensive and consuming a lot of memory as calculating the median requires all the data to be in memory at once. This is an issue in typical astrophysics calculations, which may use hundreds of thousands of FITS files. To compute the median we can use the statistics library, or the following approach:</p> <pre><code>fluxes = [17.3, 70.1, 22.3, 16.2, 20.7]\nfluxes.sort()\nmid = len(fluxes)//2\nmedian = fluxes[mid]\n# or for an even number of elements\nmedian = (fluxes[mid - 1] + fluxes[mid])/2\n</code></pre> <p>or using <code>numpy</code>: </p> <pre><code>data = load_stack(fnames)\nstack = np.dstack(data)\nmedian = np.median(stack, axis=2)\n</code></pre> <p>To avoid loading all the data in memory, we can use the binapprox algorithm to approximate the current median. The idea behind it is to find the median from the data's histogram.</p> <p>Starting from the left, if we sum up the counts in the histogram bins until we get to just over the expected mediam then we know the last bin we added must have contained the median. In fact it is better to search in bins around the standard devision of the mean. See Stacking/binapprox.py code.</p>"},{"location":"astronomy/#agn-active-galactic-nucleus","title":"AGN: Active Galactic Nucleus","text":"<p>Our eyes can only detect light and the visible part of the electromagnetic spectrum.  Galaxy has Xray, visible and radio waves. At the center of the galaxy is a black hole, which has a huge impact on the galaxy's growth and formation. In cases where there is a lot of gas in the central region of the galaxy, this material can be accreted on to the black hole via an Accretion Disk, releasing a lot of energy in the process. This is what we call, an Active Galactic Nucleus. The radiation produced by the AGN is so bright that it can outshine the entire galaxy, producing far more energy than all of the galaxy's stars combined. It may form huge jets of strong magnetic fields emanating out from around the black hole. </p> <p>Not all galaxies have AGN and not all AGN produce radio jets. While infrared radiation comes from the dusty regions that surround the black hole. The jets produce a lot of radio emissions and hot gas surrounding the central black hole region can produce x-ray emissions.</p> <p>Here is an image of combined wave lengths from visible, X-ray and radio:</p> <p></p> <p>The material that accretes onto a black hole produce X-rays, because particle are becoming very hot. We can assess the presence of supermassive black hole by measuring powerful jets coming from a compact core, rapid changes in the luminosity of the galaxy nucleus, very high speed orbital motions of stars in the galactic nucleus.</p>"},{"location":"astronomy/#cross-matching","title":"Cross-matching","text":"<p>When investigating astronomical objects, like active galactic nuclei (AGN), astronomers compare data about those objects from different telescopes at different wavelengths. This requires positional cross-matching to find the closest counterpart within a given radius on the sky.</p> <p>To create a catalog of objects from survey images, the source-finding software uses the same technics of going through all the pixels and find peaks that are statistically significant.</p> <p>How to calculate distance in the sky? Two objects in the same image are not in the same plane, we can compute the angular distance but they may be far aways on those line.</p> <p>The cross matching between 2 catalogs: The BSS catalogue lists the brightest sources from the AT20G radio survey while the SuperCOSMOS catalog lists galaxies observed by visible light surveys.</p> <p>The positions of stars, galaxies and other astronomical objects are usually recorded in either equatorial or Galactic coordinates.</p> <ul> <li>Right ascension: the angle from the vernal equinox to the point, going east along the celestial equator. Given in hours-minutes-seconds (HMS). 1 hour = 15 degrees</li> <li>Declination: the angle from the celestial equator to the point, going north (negative values indicate going south). Recorded in degrees-minutes-seconds (DMS) notation. A full circle is 360 degrees, each degree has 60 arcminutes and each arcminute has 60 arcseconds.</li> </ul> <p>The vernal equinox is the intersection of the celestial equator and the ecliptic where the ecliptic rises above the celestial equator going further east.</p> <p>To crossmatch two catalogs we need to compare the angular distance between objects on the celestial sphere, which is the projected angle between objects as seen from Earth.</p> <p>See cross-matching.py code for in place comments and study. But this program is in O(n*m), there is a Astropy library with cross marching, using k-d-tree as demonstrated in this code).</p> <p>A  k-d tree, or k-dimensional tree, is a way of representing the points in space in a recursive structure. K is the number of dimensions, which in our case are the two dimensions of our coordinate system, right ascension and declination. To construct a k-d tree, you have to recursively partition the space at the median point each time.</p> <p>The median point here in the x dimension is A. And so we split the plane at that point, and A becomes the root node of the tree. We then consider points to the left of A and split the plane in the y dimension. And again, at the median point, which is E. We repeat this process, alternating between the x and y dimensions, until the left-hand side of the tree is complete. </p> <p>Another important issue is how you can evaluate whether your matches are just chance coincidences. Or whether the galaxies have a real, physical association.  When we can measure redshift for our objects, we can establish that they're at the same distance.</p> <p>Nearly all of our radio sources have an optical counterpart, which means we can classify them into two different categories.</p> <ul> <li> <p>Most of our radio galaxies are associated with quasars. Where we're looking towards the central black hole and can see the very energetic accretion disk. The radiation from the accretion disk is so bright that it outshines all of the stars in the galaxy. And therefore, looks just like a bright star, hence the name, quasi-stellar object, or quasar.      Quasar </p> </li> <li> <p>The rest of our radio galaxies sit inside normal galaxies, where we can see a cloud of many stars grouped together. This could mean that the supermassive black hole has stopped accreting material. And the radio jets are remnants of past activity. </p> </li> </ul>"},{"location":"astronomy/#statistic-data-science-helps-astronomy","title":"Statistic / data science helps Astronomy","text":"<p>Data could not answer directly what you want to find. so we can use probability theory to assess if the data provide answer. The approach is to try to assert hypothesis and derive what kind of data we should expect to see. Then you use the fit model approach by selecting the hypothesis that fit the best the data and throw away the ones that don't fit the data.</p> <p>2016 set a record for the biggest haul of exoplanets, when the Kepler team applied statistical validation to verify over a thousand new planets.</p>"},{"location":"astronomy/#exoplanets","title":"Exoplanets","text":"<p>The science of exoplanets kicked off back in the late 1990s, the success of the space telescopes CoRoT and Kepler has really accelerated the field. Back in the 90s, we were discovering one or two planets a year, on average. </p> Kepler telescope <p>Kepler helps discover hundreds of new planets are being confirmed every year, with thousands more candidates being found.</p> Exoplanets per radion and orbital period. <p>The most common planets are the super earth. The NASA public catalog. Here are some of the helpful attributes  to consider for queries</p> Attribute Description Kepler ID Unique target identification number for stars KOI name String identifier for the Kepler Object of Interest (KOI) Teff (K) Effective temperature of a star in Kelvin Radius Radius of stars and planets in units of solar radius/earth radius respectively Kepler name Unique string identifier for a confirmed exoplanet in the planet table Period Orbital period of a planet in units of days Status Status of a discovered KOI in the planet table, e.g. \"confirmed\" or \"false positive\" Teq Equilibrium temperature of a planet in Kelvin <p>Duckdb can be used to read the exoplanet last update:</p> <pre><code>create table exoplanets as\nselect * from read_csv_auto('./cumulative_2025.10.12.csv');\n# see structure\nshow table exoplanets;\n</code></pre> <p>Some interesting queries:</p> <pre><code>SELECT koi_name, radius FROM exoplanets ORDER BY radius DESC LIMIT 5;\n# analyse the size of the unconfirmed exoplanets (kepler_name is null).\nSELECT MIN(radius), MAX(radius), AVG(radius), STDDEV(radius) FROM Planet where kepler_name is NULL;\n# how many planets in the Planet database are in a multi-planet system\nselect kepler_id, count(koi_name) from Planet group by kepler_id having count(koi_name) &gt; 1 order by count(koi_name) desc;\n</code></pre> <p>Which Earth sized planets are in the inhabitable zone of the host star?</p> <p>To work out which planets are in the habitable zone, we'll consider the energy budget of a planet. How much energy it receives from its star versus how much it radiates back into space. The intensity of the energy decrease the further the planet is from its star. The incoming energy budget of the planet clearly depends on the brightness of its star, and how close the planet is to that star.</p> <p>The insulation flux for earth is 1361 W/ m2</p>"},{"location":"coding/buildling-cli/","title":"Building CLI","text":"<p>In lot of solution project it is interesting to build custom cli to offer a simplest set of command. Design a good CLI is not easy, but since recent years there are a lot of CLIs offered by different vendors  that are very user friendly to use. The approach is also to be able to package the code as a module and also as a cli.</p> <p>They are multiple solutions to develop CLI:</p> <ul> <li>Click with Poetry</li> <li>Poetry and Typer</li> <li>UV and Typer: modern and rapid new package manager</li> </ul>"},{"location":"coding/buildling-cli/#project-structure","title":"Project structure","text":"<p>The git repository should have source code for the different components, mkdoc docs and IaC at the minimum. Then each package will have its own folder under src and tests.</p> <p>     src        /src <pre><code>tests\n</code></pre> <p>See my CLI for shift-left project</p>"},{"location":"coding/buildling-cli/#click","title":"Click","text":"<p>See Click product documentation, here are some valuable arguments for click:</p> <ul> <li>created for the Flask project</li> <li>argparse does not allow proper nesting of commands</li> <li>Commands can be attached to other commands of type Group. Flexible way to add command to groups</li> </ul>"},{"location":"coding/buildling-cli/#typer","title":"Typer","text":"<p>Product main page and documentation:</p> <ul> <li>It is based on Click</li> <li>Use same design as FastAPI</li> </ul>"},{"location":"coding/buildling-cli/#a-project-using-uv-typer-and-click","title":"A project using uv, Typer and click","text":"<p>This section is a quick summary of what may be done to build a CLI for the shift-left project for flink-test-harness CLI.</p>"},{"location":"coding/buildling-cli/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>uv cli for package managementfor installation</li> <li>The cli project is a subproject of a bigger repository.</li> </ul>"},{"location":"coding/buildling-cli/#design-considerations","title":"Design considerations","text":"<ul> <li>Develop foundation modules use by the cli and to package as an independent pypi library.</li> <li>Unit test modules</li> <li>Unit test cli</li> </ul>"},{"location":"coding/buildling-cli/#steps","title":"Steps","text":"<ul> <li>Under the main project src folder, initialize a new uv project:</li> </ul> <pre><code>uv init --app --package flink-test-harness\n# --app tells uv that we will write an application\n# --package tells uv that we will want to build a package out of our code to distribute it \n</code></pre> <ul> <li>In the src/flink-test-harness/main.py add the Typer based commands to define the basic cli. See product documentation.</li> <li>Be sure to have the good alias and references in the <code>pyproject.toml</code> file</li> </ul> <pre><code>[project.scripts]\nflink-test-harness = \"flink_test_harness.main:app\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/flink_test_harness\"]\n</code></pre> <ul> <li>Run unit test for the module</li> </ul> <pre><code>uv run tests/core/test_inventory_mgr.py\n</code></pre> <ul> <li>Run unit test for the cli</li> </ul> <pre><code>uv run tests/cli/test_project_cli.py\n</code></pre> <ul> <li>Run the cli using uv</li> </ul> <pre><code>uv run flink-test-harness greet jerome\n# other example\nuv run shit_left\n</code></pre> <p>See shift_left project</p> <ul> <li>Build the module</li> </ul> <pre><code>uv build\n</code></pre> <ul> <li>To install the tool locally with uv</li> </ul> <pre><code>uv tool install . -e  \n</code></pre> <ul> <li>test the installed tool</li> </ul> <pre><code>shift_left --help\n</code></pre> <ul> <li>uninstall</li> </ul> <pre><code>uv tool list\nuv tool uninstall shift-left \n</code></pre> <ul> <li>To build the wheel packaging</li> </ul> <pre><code>uv build\n</code></pre> <ul> <li>Share the wheel and install in a python env via pip or uv</li> </ul>"},{"location":"coding/buildling-cli/#some-tricks","title":"Some tricks","text":"<ul> <li>Get the full documentation in markdown of all the options</li> </ul> <pre><code>typer src/shift_left/cli.py utils docs\n</code></pre>"},{"location":"coding/dev-env/","title":"Development environments","text":"<p>Apple Mac OS uses Python for its own operations, so it is very important to isolate the development environment from the operation of the OS to avoid compromising the integrity of the whole system. So virtual env must be used.</p>"},{"location":"coding/dev-env/#virtual-env","title":"Virtual env","text":"<p>The goals are:</p> <ul> <li>avoid installing softwares and libraries not often used on the native OS</li> <li>describe the dependencies on library so programs developed 5 years ago should still run</li> <li>easy to switch laptop</li> <li>quicker provisioning than a VM running with Vagrant, still offering mounting host folder, running under localhost</li> <li>use docker compose for each project to manage component dependencies</li> <li>if needed pipenv can be used to set up virtual environment to isolate dependencies</li> </ul> <p>Classical venv creation:</p> <pre><code>python -m venv .venv\n# for MAC / Linux users\nsource ./venv/bin/activate\n# for Windows\nsource ./venv/Scripts/activate\n</code></pre> <p>Then for each projects define a requirement.txt</p>"},{"location":"coding/dev-env/#use-the-different-docker-images","title":"Use the different docker images","text":"<p>The DockerfileForEnv in the current project defines an image for running python 3.7 with Flask, pytest, panda and other basic libraries.</p> <p>To build the image you need docker engine and do the following</p> <pre><code>docker build -t jbcodeforce/python37 . \n</code></pre> <p>Then start the image as container with the command below, which also mount your local folder to the <code>/home</code> folder inside the docker container:</p> <pre><code>docker run -e DISPLAY=192.168.1.89:0 --name pythonenv -v $(pwd):/home/ -it --rm -p 5000:5000 jbcodeforce/python37 bash\n</code></pre> <p>Note</p> <pre><code>The script named `startPythonDocker.sh` performs this command.\n</code></pre> <p>The docker image includes <code>pipenv</code> for improving the dependency management. </p> <p>The other Dockerfile for astrophysic is Here</p>"},{"location":"coding/dev-env/#using-graphics-inside-the-python-container","title":"Using graphics inside the python container","text":"<p>The approach is to run graphics program inside python interpreter, but the windows will appear on the host machine (the Mac). To do so we need a bidirectional communication between the docker container and the Mac. This is supported by the <code>socat</code> tool. To install it the first time do the following:</p> <pre><code>brew install socat\n</code></pre> <p>When installed, open a new terminal and start socat with the command:</p> <pre><code>socat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\\"$DISPLAY\\\"\n</code></pre> <p>As the container is running X window system, we need to also install a X system on Mac. This is done via the <code>Xquartz</code> program:</p> <pre><code>brew install xquartz\n</code></pre> <p>Then start Xquartz from the application or using</p> <pre><code>open -a Xquartz\n</code></pre> <p>A white terminal window will pop up. The first time Xquartz is started,  open up the <code>preferences</code> menu and go to the <code>security</code> tab. Then select \u201callow connections from network clients\u201d to check it <code>on</code>.</p> <p>See this note from Nils De Moor for more information.</p>"},{"location":"coding/dev-env/#use-pipenv","title":"Use pipenv","text":"<p>Pipenv offers the last best practices from other language to manage virtual environment and dependencies for Python. Adding and removing packages is also updating the dependencies descriptor file: Pipfile. It basically combine pip and virtualenv. It helps addressing build inconsistency that requirements.txt brings.</p> <p>To install it on the mac:</p> <pre><code>brew install pipenv\n</code></pre> <p>When using the docker image you do not need to install pipenv on the host. It is also available in the docker image so the following commands should work from the bash inside the docker container.</p> <pre><code># Create a new project using Python 3.7\npipenv --python 3.7\n# start the virtual env shell\npipenv shell\n# or start a python interpreter\npipenv run python\n# or run a program with python interpreter\npipenv run python FaustEasiestApp.py\n# install dependencies including dev\npipenv install --dev\n#Check your installed dependencies for security vulnerabilities:\npipenv check\n# print a pretty graph of all your installed dependencies.\npipenv graph \n# Uninstalls all packages not specified in Pipfile.lock.\npipenv clean\n# lock dependencies\npipenv lock\n</code></pre>"},{"location":"coding/dev-env/#run-the-python-interpreter","title":"Run the python interpreter","text":"<p>Start <code>python</code> in the container shell:</p> <pre><code>root@485cff8245df:/home#  python\nPython 3.7.4 (default, Jul  9 2019, 00:06:43) \n[GCC 6.3.0 20170516] on linux\n&gt;&gt;&gt; \n</code></pre> <p>Use <code>exit()</code> to get out of the python interpreter, and Ctrl D for getting out of the Docker container.</p>"},{"location":"coding/dev-env/#vscode","title":"vscode","text":"<p>To start a new Python project:</p> <ol> <li>create folder on the OS file system, then start <code>code &lt;created_dir_name&gt;</code></li> <li>Verify Python extensions: The microsoft extension includes debugger and pylance. Install also pylint as a linter.</li> <li> <p>In case of change the Python environment: in footer bar </p> </li> <li> <p>Product doc</p> </li> <li> <p>Tricks</p> </li> <li> <p>Ctrl+shift P to open command palette </p> </li> <li>Ctrl K + ctrl T for changing the theme for all windows</li> </ol> <p>Settings are at user level, so at the workspaces and windows level, or at workspace level.</p> <ul> <li>Command short cut sheet Windows mac</li> <li>Article on theme customization per workspace and theme color</li> </ul>"},{"location":"coding/dp/","title":"Design Patterns","text":"<p>Here are some examples on how to implement some classical design pattern in Python.</p>"},{"location":"coding/dp/#singleton","title":"Singleton","text":"<p>Singleton Pattern ensures that only one instance of a class exists and provides a global point of access to it.</p> <pre><code>class _Glossary:\n    _instance = None\n\n    # method for the singleton class....\n\ndef Glossary(path:str ) -&gt; _Glossary:\n    \"\"\"Factory method to access to the unique instance\"\"\"\n    if _Glossary._instance is None:\n        _Glossary._instance = _Glossary()\n        _Glossary._instance.load_glossary(path)\n    return _Glossary._instance\n</code></pre> <p>Access to a singleton from a FastAPI app using dependency injection for example</p> <pre><code>def get_db() -&gt; Optional[Database]:\n    return Database()\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root(db: Optional[Database] = Depends(get_db)):\n    if db:\n        result = db.db.my_collection.find_one()\n        return {\"message\": result}\n</code></pre>"},{"location":"coding/dp/#dependency-injection","title":"Dependency injection","text":""},{"location":"coding/references/","title":"Code references","text":""},{"location":"coding/references/#basics","title":"Basics","text":"<ul> <li>firstinput.py for reading user input</li> <li>Variable scope between global, local,...</li> <li>travis.py to play with lists, for in range() and conditions</li> <li>cinema.py to illustrate how to use for dictionary</li> <li>Play with data structures: lists, queues, matrix, sets, and more dictionaries, with how to navigate into those structures</li> <li>Reverse a word and add aye, use loops, break, in voyals...</li> <li>Object Oriented Python: classes and inheritance: using constructor (init()) and method with self argument.</li> <li>Modules, import, and packages. Do not forget to set PYTHONPATH to the root folder to access any new modules</li> </ul>"},{"location":"coding/references/#flask","title":"Flask","text":"<ul> <li>Flask web app hello world then REST API end point with Flask and staticApp.py</li> <li>Flask serving a simple angular App</li> <li>TDD with Flask app and docker from testdriven.io course</li> </ul>"},{"location":"coding/references/#algorithms","title":"Algorithms","text":"<ul> <li>Sorting arrays: Bubblesort, selection sort, insertion sort and quicksort.</li> <li>Binary Tree with InOrderTraversal, PreOrderTraversal, PostOrderTraversal.</li> <li>Binary search within a sorted array which is a divide and conquer algorithm.</li> <li>Depth First Search, graph, Breadth First Search DFS: explores the highest-depth nodes first before being forced to backtrack and expand shallower nodes. BFS: explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.</li> </ul>"},{"location":"coding/references/#graphics","title":"Graphics","text":"<ul> <li>Use a simple graphics API to create a window, draw circle and move them.</li> <li>Plotting normal curve with matplotlib</li> </ul>"},{"location":"coding/references/#web-scrawling","title":"Web scrawling","text":"<p>Use urllib and beautiful soup to remove html tags from a web page to get text to parse. See this note for guidances</p> <ul> <li>Use regular expression (re module) to extract number from a text read from a file.</li> </ul>"},{"location":"coding/references/#astronomy","title":"Astronomy","text":"<p>See detailed note here and code is under <code>astronomy</code> folder.</p>"},{"location":"coding/references/#aws","title":"AWS","text":"<p>To get some sample code to use AWS SDK see this folder.</p>"},{"location":"coding/references/#unit-testing","title":"Unit testing","text":"<ul> <li>unittest</li> <li>Pytest framework</li> <li>[moto for backend mockup]http://docs.getmoto.org/en/latest/index.html)</li> </ul>"},{"location":"coding/uv/","title":"uv","text":"<p>uv is an extremely fast Python package (to replace pip) and project manager (for virtual env). </p>"},{"location":"coding/uv/#value-propositions","title":"Value propositions","text":"<ul> <li>uv manages project dependencies and environments.</li> <li>uv installs different Python version and allows switching between versions.</li> <li>optimize dependencies caching and avoid deduplication</li> </ul> <p>See features list and basic commands</p>"},{"location":"coding/uv/#getting-started","title":"Getting started:","text":"<ul> <li>installation</li> <li> <p>Common commands:     <pre><code>uv python install: Install Python versions.\nuv python list: View available Python versions.\nuv python find: Find an installed Python version.\nuv run: Run a script.\nuv add --script: Add a dependency to a script\nuv remove --script: Remove a dependency from a script\nuv self update: upgrade version\n</code></pre></p> </li> <li> <p>Work with virtual environment <pre><code>uv venv\nuv venv --python 3.13\nuv python pin 3.13\ndeactivate\n</code></pre></p> </li> <li> <p>Working with Python project using <code>uv init</code>.</p> <ul> <li>See understanding project type</li> <li>pyproject.toml guide.</li> <li>Manage dependencies</li> </ul> </li> <li> <p>Work with tools to publish as python package     <pre><code>uv tool run: Run a tool in a temporary environment.\nuv tool install: Install a tool user-wide.\nuv tool uninstall: Uninstall a tool.\nuv tool list: List installed tools.\nuv tool update-shell: Update the shell to include tool executables.\n</code></pre></p> </li> </ul>"},{"location":"coding/uv/#some-interesting-things-to-do","title":"Some interesting things to do","text":"<ul> <li>Check version: <code>uv version</code></li> <li>Use different python version</li> <li>Use / create project (create a <code>pyproject.toml</code>) See project guide.</li> <li>Run script in the context of a project to get access to modules: <code>uv run python ...</code></li> <li>Manage dependencies: <code>uv add --script</code> or <code>uv remove --script</code></li> <li>Full CLI reference</li> </ul>"},{"location":"coding/uv/#projects-i-am-using-it","title":"Projects I am using it","text":"<ul> <li>Move to real-time with Shift left to develop a CLI</li> </ul>"},{"location":"data/pandas/","title":"Pandas summary","text":"<p>See the Kaggle quick tutorial on pandas.</p> <p>Pandas has two core objects: the DataFrame and the Series.</p> <p>DataFrame is like a table which contains an array of individual entries,  each of which has a certain value. Each entry corresponds to a row (or record) and a column.</p> <p>Series is a sequence of data values, it may be a single column of a DataFrame.</p> <p>Here is a quick summary of some of the tutorial content:</p> <pre><code># use pandas\nimport pandas as pd\n# Create a data frame from a dictionary whose keys are the column names and values are list of entries\npd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']})\n# with row header as index\npd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], \n              'Sue': ['Pretty good.', 'Bland.']},\n             index=['Product A', 'Product B'])\n# read from file\nhome_data = pd.read_csv(a_file_path,index_col=0)\n# get row / columns size\nhome_data.shape\nhome_data.head()\n# Series\npd.Series([\"4 cups\", \"1 cup\", \"2 large\", \"1 can\"],index=[\"Flour\", \"Milk\", \"Eggs\", \"Spam\"],name=\"Dinner\")\n</code></pre>"},{"location":"data/pandas/#indexing","title":"Indexing","text":"<p>Pandas uses two approach:</p> <ul> <li>index-based selection: To select first row of a data frame: <code>reviews.iloc[0]</code>.  To get a column with iloc use: <code>reviews.iloc[:, 0]</code>. We can select row too, like getting  the last five elements of the dataset: <code>reviews.iloc[-5:]</code></li> <li>label-based selection. gets from data index value, not its position: <code>reviews.loc[0, 'country']</code>.  Or select three columns and all rows: <code>reviews.loc[:, ['taster_name', 'taster_twitter_handle', 'points']]</code> </li> </ul> <p>Label-based selection derives its power from the labels in the index. We can set index with:</p> <pre><code>reviews.set_index(\"title\")\n\n# Select elements that match condition \nreviews.loc[reviews.country == 'Italy']\n# or within a list\nreviews.loc[reviews.country.isin(['Italy', 'France'])]\n# not empty cell\nreviews.loc[reviews.price.notnull()]\n# get specific rows\nsample_reviews = reviews.iloc[[1,2,3,5,8],:]\n# Combining conditions\ntop_oceania_wines = reviews.loc[reviews.country.isin(['New Zealand','Australia']) &amp; (reviews.points &gt;= 95)]\n</code></pre> <pre><code># Get high-level summary of the attributes of the given column\nreviews.points.describe()\n# Get unique elements in a column\nreviews.taster_name.unique()\n# To see a list of unique values and how often they occur in the dataset\nreviews.taster_name.value_counts()\n# Ex bargain_wine with the title of the wine with the highest points-to-price ratio in the dataset.\nbargain_idx = (reviews.points / reviews.price).idxmax()\nbargain_wine = reviews.loc[bargain_idx, 'title']\n</code></pre>"},{"location":"data/pandas/#map","title":"map","text":"<p>Map() takes one set of values and \"maps\" them to another set of values. <code>map()</code> should expect  a single value from the Series and return a transformed version of that value.</p> <pre><code>review_points_mean = reviews.points.mean()\nreviews.points.map(lambda p: p - review_points_mean)\n# build a series to count how many times each of tropical, fruity words appears in the description column in the dataset.\ntopic= reviews.description.map(lambda d: \"tropical\" in d).sum()\nfruit= reviews.description.map(lambda d: \"fruity\" in d).sum()\ndescriptor_counts = pd.Series([topic,fruit], index=['tropical', 'fruity'])\n</code></pre> <p><code>apply()</code> transforms a whole DataFrame by calling a custom method on each row.</p> <p>Both methods don't modify the original data they're called on.</p>"},{"location":"data/pandas/#grouping","title":"Grouping","text":"<p>Use <code>groupby()</code> to group our data, and then do something specific to the group the data is in:</p> <pre><code># Group of reviews which allotted the same point values to the given wines. \n# Then, for each of these groups, grab the `points` column and count how many times it appeared\nreviews.groupby('points').points.count()\n# Apply transformation to the new dataframe\nreviews.groupby('winery').apply(lambda df: df.title.iloc[0])\n</code></pre> <p><code>agg()</code> lets us run a bunch of different functions on our DataFrame simultaneously</p> <pre><code>reviews.groupby(['country']).price.agg([len, min, max])\n</code></pre> <p>groupby combined with some specific operations may create multi-indexes, which looks like a tiered structure.</p> <pre><code>countries_reviewed = reviews.groupby(['country', 'province']).description.agg([len])\n# sort by ascending\ncountries_reviewed.sort_values(by='len', ascending=False)\n# To sort by index values, use the companion method sort_index()\n# sort by more than one column\ncountries_reviewed.sort_values(by=['country', 'len'])\n# most wine reviewer, using their twitter name\nreviews.groupby(['taster_twitter_handle']).taster_twitter_handle.count()\n# What is the best wine I can buy for a given amount of money? \nbest_rating_per_price = reviews.groupby('price').points.max().sort_index()\n# What are the minimum and maximum prices for each variety of wine? \nprice_extremes = reviews.groupby('variety').price.agg([min,max])\n# What are the most expensive wine varieties?\nsorted_varieties = reviews.groupby('variety').price.agg([min,max]).sort_values(by=['min','max'],ascending=False)\n# A series with index is reviewers and whose values is the average review score given out by that reviewer.\nreviewer_mean_ratings = reviews.groupby(['taster_name']).points.agg('mean')\n</code></pre>"},{"location":"data/pandas/#data-types","title":"Data types","text":"<pre><code># Get data type of the points column\nreviews.points.dtype\n# All column types\nreviews.dtypes\n# convert to another type, when it makes sense\nreviews.points.astype('float64')\n</code></pre>"},{"location":"data/pandas/#missing-values","title":"Missing values","text":"<p>Entries missing values are given the value NaN, short for \"Not a Number\". </p> <pre><code>reviews[pd.isnull(reviews.country)]\n# How many reviews in the dataset are missing a price?\nmissing_prices = reviews[reviews.price.isnull()]\nn_missing_prices = len(missing_prices)\n\n# What are the most common wine-producing regions? \n# Series counting the number of times each value occurs in the region_1 field. \n# This field is often missing data, so replace missing values with Unknown. \n# Sort in descending order.\nreviews.region_1.fillna('Unknown').value_counts().sort_values(ascending=False)\n\n# Replace a value by another\nreviews.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\")\n</code></pre>"},{"location":"data/pandas/#renaming","title":"Renaming","text":"<p><code>rename()</code> lets you rename index or column values by specifying a index or column keyword parameter, respectively. </p> <pre><code>reviews.rename(columns={'points': 'score'})\n\nreviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})\n\n# Rename index\nreindexed = reviews.rename_axis('wines',axis='rows')\n</code></pre>"},{"location":"data/pandas/#combining","title":"Combining","text":"<p>Pandas has three core methods for doing combining data frames. In order of increasing complexity, these are <code>concat(), join()</code>, and <code>merge()</code>.</p> <p>join() lets you combine different DataFrame objects which have an index in common</p> <pre><code>left = canadian_youtube.set_index(['title', 'trending_date'])\nright = british_youtube.set_index(['title', 'trending_date'])\n\nleft.join(right, lsuffix='_CAN', rsuffix='_UK')\n</code></pre> <p>The lsuffix and rsuffix parameters are necessary here because the data has the same column names in both British and Canadian datasets.</p>"},{"location":"data/webcrawling/readme/","title":"Web crawling examples","text":""},{"location":"data/webcrawling/readme/#requests","title":"Requests","text":"<p>Requests is one modern library to do http calls. </p> <p>The Response object includes return code and text or can be transform using json() method.</p>"},{"location":"data/webcrawling/readme/#using-python-multiple-threading-for-assessing-the-most-popular-image-on-imgurcom","title":"Using python multiple threading for assessing the most popular image on imgur.com","text":"<p>This is the implementation from this article from MARCUS MCCURDY</p> <p>The app was registered in imgur using this link. The client id is set in environment variable:</p> <pre><code>IMGUR_CLIENT_ID 85b0a015a03ea5798f2572bd6c47b6bd935ec090a10b7d4c1a31378\n</code></pre> <p>The code is under <code>web_data/imgur</code> folder. The module <code>single.py</code> is loading one image at a time. <code>DownloadWorker.py</code> implements a class which uses <code>Thread</code> to run the download in parallel. </p> <p>The script creates 8 threads and a queue to get the links from where to download the image. The run method has been overridden, which runs an infinite loop. On every iteration, it calls <code>self.queue.get()</code> to try and fetch a URL to from a thread-safe queue. It blocks until there is an item in the queue for the worker to process. </p> <p>Once the worker receives an item from the queue, it then calls the same <code>download_link()</code> function, when the download is finished, the worker signals the queue that the task is done. This is very important, because the Queue keeps track of how many tasks were enqueued. The call to <code>queue.join()</code> would block the main thread forever if the workers did not signal that they completed a task.</p>"},{"location":"fastapi/","title":"FastAPI","text":"<p>FastAPI helps to build backend REST api in python.</p>"},{"location":"fastapi/#boilerplate","title":"Boilerplate","text":"<ul> <li>Create a main with health and metrics routes</li> <li>Add any specific routes for a business entities in its own router</li> <li>Add dependencies to initialize singletons, reference as part of the dependencies</li> </ul>"},{"location":"fastapi/#running","title":"Running","text":"<p>To start an app, use uvicorn in a shell script</p> <pre><code>uvicorn orchestrator_api:app --host 0.0.0.0 --port 8000 --reload\n</code></pre> <p>Or add this in the main server python code:</p> <pre><code>import uvicorn\n\nif __name__ == \"__main__\":\n     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> <ul> <li>Some URLs</li> </ul> <pre><code>http://127.0.0.1:8000/docs\nhttp://127.0.0.1:8000/redoc\n</code></pre>"},{"location":"fastapi/#how-tos","title":"How tos","text":"<ul> <li>A server with a websocket listener</li> <li>Test to upload file and pydantic object in the same URL</li> <li>Expose async function to stream content with a generator. It uses yield and asyncio</li> </ul>"},{"location":"fastapi/#some-content-to-read","title":"Some content to read","text":"<ul> <li>See full tutorial</li> </ul>"},{"location":"flask/flask-tdd-docker/","title":"Flask microservice with TDD and docker","text":"<p>This content is based on the tutorial from tesdriven.io and covers:</p> <ul> <li>pipenv for virtual environment and dependencies management</li> <li>Flask Restful where resources are build on top of Flask views</li> <li>Flask CLI tool to run and manage the app from the command line.</li> <li>Debugging in development mode</li> <li>Docker for python developer</li> <li>Flask-sqlalchemy to support SQLAlchemy in Flask</li> <li>Psycopg is the most popular PostgreSQL adapter for the Python Here is a quick summary of things learnt.</li> <li>Postgresql docker image</li> <li>Pytest for unit and functional testing</li> <li>Blueprints for organizing code and components</li> </ul> <p>The folder Flask/flask-tdd-docker includes the training code.</p>"},{"location":"flask/flask-tdd-docker/#set-virtual-env","title":"Set virtual env","text":"<p>The old way to define virtual environment was to use the following approach:</p> <pre><code>python3.7 -m venv env\nsource env/bin/activate\n# play with python ...\n# Stop with:\ndeactivate\n</code></pre> <p>As of today, the approach is to use <code>pipenv</code>, where you update the project and development dependencies in a <code>Pipfile</code>. </p> <pre><code>pipenv --python 3.7\n# start the virtual env\npipenv shell\npipenv install --dev\n</code></pre> <p>Freeze the dependencies:</p> <pre><code>pipenv lock -r &gt; requirements.txt\n</code></pre>"},{"location":"flask/flask-tdd-docker/#define-and-run-the-flask-app","title":"Define and run the flask app","text":"<p>Define a manage.py to represent the app, and use the Flask CLI shell to manage the app from command line:</p> <pre><code>from flask.cli import FlaskGroup\nfrom project import app\n\n\ncli = FlaskGroup(app)\n\n\nif __name__ == '__main__':\n    cli()\n</code></pre> <pre><code>export FLASK_APP=project/__init__.py\n# use the Flask CLI from inside the app itself\npython manage.py run\n</code></pre> <p>Run in development mode for debugging.</p> <pre><code>export FLASK_ENV=development\npython manage.py run\n* Serving Flask app \"project/__init__.py\" (lazy loading)\n* Environment: development\n* Debug mode: on\n</code></pre> <p>With the Flask shell we can explore the data in the application:</p> <pre><code>flask shell\n</code></pre>"},{"location":"flask/flask-tdd-docker/#using-docker-and-docker-compose","title":"Using docker and docker compose","text":"<p>The dockerfile uses alpine linux and non root user. The docker compose uses volume to mount the code into the container. This is a must for a development environment in order to update the container whenever a change to the source code is made. Then build the image using docker compose.</p> <pre><code>docker-compose build\n# then start in detached mode\ndocker-compose up -d\n# Rebuild the docker images \ndocker-compose up -d --build\n# Access app logs\ndocker-compose logs\n# Access to a python shell to control the flask app\ndocker-compose exec users flask shell\n</code></pre>"},{"location":"flask/flask-tdd-docker/#add-persistence-on-postgresql-and-use-sqlalchemy","title":"Add persistence on Postgresql and use SQLAlchemy","text":"<p>To initialize the postgresql copy a sql file under /docker-entrypoint-initdb.d (creating the directory if necessary). </p> <p>docker compose section for postgresql:</p> <pre><code>users-db:  \n    build:\n      context: ./project/db\n      dockerfile: Dockerfile\n    expose:\n      - 5432\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n</code></pre> <p>Once spun up, PostgreSQL will be available on port 5432. Be sure to include dependencies in the app dockerfile</p> <pre><code># install dependencies\nRUN apk update &amp;&amp; \\\n    apk add --virtual build-deps gcc python-dev musl-dev &amp;&amp; \\\n    apk add postgresql-dev &amp;&amp; \\\n    apk add netcat-openbsd &amp;&amp; \\\n    pip install --upgrade pip &amp;&amp; \\\n    pip install --upgrade --user pipenv \n</code></pre> <p>Also to avoid having the application getting error because it could not contact the database add a <code>entrypoint.sh</code> shell to loop until the database is accessible before starting the python app.</p> <p>To access <code>psql</code>, use the following docker compose command</p> <pre><code>docker-compose exec users-db psql -U postgres\n\npsql (11.4)\nType \"help\" for help.\n\npostgres=# \\c users_dev\nYou are now connected to database \"users_dev\" as user \"postgres\".\nusers_dev=# \\dt\nDid not find any relations.\nusers_dev=# \\q\n</code></pre> <p>In the <code>manage.py</code> file, register a new flask CLI command, <code>recreate_db</code>, so that we can run it from the command line like:</p> <pre><code>docker-compose exec users python manage.py recreate_db\n</code></pre> <pre><code># @cli.command('recreate_db')\ndef recreate_db():\n    db.drop_all()\n    db.create_all()\n    db.session.commit()\n</code></pre>"},{"location":"flask/flask-tdd-docker/#add-tests-with-pytest","title":"Add tests with pytest","text":"<p>While unittest requires test classes, Pytest just requires functions to get up and running.</p> <p>Define fixtures as reusable elements for future tests</p> <p>They have a scope associated with them, which indicates how often the fixture is invoked:</p> <ul> <li>function - once per test function</li> <li>class - once per test class</li> <li>module - once per test module</li> <li>session - once per test session</li> </ul> <p>Some fixture execution guidance</p> <p>Define python script using 'test_' or '_test.py'. Here is an example of functional testing: <pre><code>def test_ping(test_app):\n    client = test_app.test_client()\n    resp = client.get('ping')\n    data = json.loads(resp.data.decode())\n    assert resp.status_code == 200\n    assert 'pong' in data['message']\n    assert 'success' in data['status']\n</code></pre></p> <p>Execute test with pytest: <code>pytest project/tests/</code> or with docker compose</p> <pre><code>docker-compose exec users pytest \"project/tests\"\n</code></pre>"},{"location":"flask/flask-tdd-docker/#test-coverage","title":"Test coverage","text":"<p>Coverage.py is a popular tool for measuring code coverage in Python-based applications. Now, since we're using Pytest, we'll integrate Coverage.py with Pytest using pytest-cov. In <code>Pipfile</code> add <code>pytest-cov = \"&gt;=2.7.1\"</code> then do <code>pipenv install</code></p> <p>Then once the image is rebuilt, run the following command to assess the test coverage:</p> <pre><code>docker-compose exec users pytest \"project/tests\" -p no:warnings --cov=\"project\"\n# or using html page\ndocker-compose exec users pytest \"project/tests\" -p no:warnings --cov=\"project\" --cov-report html\n</code></pre> <p>Remember: just because you have 100% test coverage doesn\u2019t mean you're testing the right things</p>"},{"location":"flask/flask-tdd-docker/#code-quality","title":"Code quality","text":"<p>Linting is the process of checking your code for stylistic or programming errors. Although there are a number of commonly used linters for Python, we'll use Flake8 since it combines two other popular linters -- pep8 and pyflakes.</p> <p>In Pipfile add <code>flake8 = \"&gt;=3.7.8\"</code>, do a <code>pipenv install</code> then freeze the dependencies with <code>pipenv lock -r &gt; requirements.txt</code>, then rebuild the docker image and run flake8:</p> <pre><code> docker-compose exec users flake8 project\n</code></pre> <p>Black helps to format code and apply code formatting:</p> <pre><code># check\ndocker-compose exec users black project --check\n# see the propose changes\ndocker-compose exec users black project --diff\n# apply the change\ndocker-compose exec users black project\n</code></pre>"},{"location":"flask/flask-tdd-docker/#add-blueprints-template","title":"Add Blueprints template","text":"<p>Blueprints are self-contained components, used for encapsulating code, templates, and static files. They are apps within the app. For example REST resource can be defined in Blueprint.</p> <p>For example to add an api and a resource, define a new py file, and create a blueprint instance:</p> <pre><code>users_blueprint = Blueprint('users', __name__)\napi = Api(users_blueprint)\n</code></pre> <p>Then define a class with functions to support the expected Resource, and add this class to a url to the api.</p> <pre><code>class UsersList(Resource):\n    def get(self):\n        ...\n    def post(self):\n        ...\napi.add_resource(UsersList, '/users')\n</code></pre> <p>Finally register the resouce to the flask application:</p> <pre><code>    from project.api.users import users_blueprint\n    app.register_blueprint(users_blueprint)\n</code></pre> <p>See the code in users.py and init.py</p> <p>Factory to create an app needs to be named <code>create_app</code>. </p>"},{"location":"flask/flask-tdd-docker/#adding-admin-and-model-view","title":"Adding admin and model view","text":""},{"location":"flask/flask-tdd-docker/#production-deployment-with-gunicorn","title":"Production deployment with gunicorn","text":"<p>Create a specific <code>Dockerfile.prod</code> and set the environment variable to run Flask in production mode and use gunicorn as container, and run under a user that is not root. </p> <pre><code>ENV FLASK_ENV production\nENV APP_SETTINGS project.config.ProductionConfig\n\n# add and run as non-root user\nRUN adduser -D myuser\nUSER myuser\n\n# run gunicorn\nCMD gunicorn --bind 0.0.0.0:$PORT manage:app\n</code></pre>"},{"location":"flask/flask-tdd-docker/#heroku","title":"Heroku","text":"<p>Using heroku CLI.</p> <pre><code>$ heroku login\n# create a app\n$ heroku create \nCreating app... done, murmuring-shore-37331\nhttps://murmuring-shore-37331.herokuapp.com/ | https://git.heroku.com/murmuring-shore-37331.git\n\n# login to docker private registry\n$ heroku container:login\n\n# create a postgresql with the hobby-dev plan\n$ heroku addons:create heroku-postgresql:hobby-dev --app murmuring-shore-37331\n\nCreating heroku-postgresql:hobby-dev on murmuring-shore-37331... free\nDatabase has been created and is available\n ! This database is empty. If upgrading, you can transfer\n ! data from another database with pg:copy\nCreated postgresql-horizontal-04149 as DATABASE_URL\nUse heroku addons:docs heroku-postgresql to view documentation\n\n# Get database URL\nheroku config:get DATABASE_URL --app murmuring-shore-37331\n</code></pre> <p>The containers used at Heroku are called \u201cdynos.\u201d Dynos are isolated, virtualized Linux containers that are designed to execute code based on a user-specified command.</p> <p>To build an image for the docker private registry, using the web dyno, that is free.</p> <pre><code>$ docker build -f Dockerfile.prod -t registry.heroku.com/murmuring-shore-37331/web .\n# publish\n$ docker push registry.heroku.com/murmuring-shore-37331/web:latest\n# test locally\n$ docker run --name flask-tdd -e \"PORT=8765\" -p 5002:8765 registry.heroku.com/murmuring-shore-37331/web:latest\n\n# Release the image, meaning the app will be based on the container image\n$ heroku container:release web --app murmuring-shore-37331\nReleasing images web to murmuring-shore-37331... done\n</code></pre> <p>Once the image is \"released\", the app is accessible via <code>https://murmuring-shore-37331.herokuapp.com/ping</code></p> <p>Access to logs: <code>heroku logs --app murmuring-shore-37331</code></p> <p>The users are not yet created, so we can run the CLI <code>heroku run</code>:</p> <pre><code># create DB\nheroku run python manage.py recreate_db --app murmuring-shore-37331\n# populate the data\nheroku run python manage.py seed_db --app murmuring-shore-37331\n# Access the database with psql: \n# 1. start a local docker postgresql with psql\ndocker run -ti postgresql bash\n&gt; psql postgres://....\n&gt; PSQL:\n</code></pre>"},{"location":"flask/readme/","title":"Python Flask Studies","text":"<p>The most complete starter code is from the Flask TDD tutorial and using docker.  But I have incremental apps, to make it simpler to develop an app from scratch.</p>"},{"location":"flask/readme/#some-concepts","title":"Some concepts","text":"<p>Flask app takes care of dispatching requests to views and routes.</p>"},{"location":"flask/readme/#samples","title":"Samples","text":"<p>To use a boiler plate code with Flask, Blueprint, Swagger, Prometheus see the boiler plate folder.</p>"},{"location":"flask/readme/#the-simplest-flask-app","title":"The simplest Flask app","text":"<p>The simplest Flask app is presented in the quickstart and the matching code is under Flask/firstApp/firstApp.py. To execute it in your python environment:</p> <pre><code>cd Flask/firstApp\n# start docker image for dev environment\ndocker run -ti -v $(pwd):/app -p 5000:5000 jbcodeforce/python37 bash\n# Can run it with python - it will start in debug mode\npython firstApp.py\n# Or run it with flask CLI\nexport FLASK_APP=firstApp.py\nflask run --host=0.0.0.0\n * Serving Flask app \"firstApp\"\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [17/Dec/2018 20:49:42] \"GET / HTTP/1.1\" 200 -\n</code></pre> <p>The FLASK_APP environment variable is the name of the module to import at <code>flask run</code>.</p> <p>To make the server publicly available simply by adding --host=0.0.0.0 to the command: <code>flask run --host=0.0.0.0</code></p> <p>If we want to run it in debug mode then any change to the code reload itself. To do so use: </p> <pre><code>export FLASK_ENV=development\nflask run --host=0.0.0.0\n</code></pre> <p>Next, is to use gunicorn to run it on top of a <code>wsgi</code> server so in the docker container add:</p> <pre><code>gunicorn -w 4 -b 0.0.0.0:5000 firstApp:app\n</code></pre> <p>Which is the command in the dockerfile under the firstApp folder:</p> <pre><code> docker build -t jbcodeforce/firstApp .\n</code></pre> <p>Start the image with</p> <pre><code>docker run --name firstApp --rm -p 5000:5000 jbcodeforce/firstApp\n</code></pre>"},{"location":"flask/readme/#serving-static-pages","title":"Serving static pages","text":"<p>Add a folder named static at the same level as app to start. The staticApp.py demonstrates the routing specified and the api to send the file.</p> <pre><code>from flask import Flask\napp = Flask(__name__)\n\n\n@app.route('/')\ndef root():\n    return app.send_static_file('404.html')\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True,host='0.0.0.0')\n</code></pre> <p>and the execution:</p> <pre><code>export FLASK_APP=staticApp.py\nflask run\n * Serving Flask app \"staticApp\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [17/Dec/2018 21:29:00] \"GET / HTTP/1.1\" 200 -\n</code></pre>"},{"location":"flask/readme/#a-rest-api","title":"A REST api","text":"<p>The route decorator is used to bind function to a URL. You can add variables and converter. The <code>firstRESTApp.py</code> illustrates the different patterns. The important modules to import are:</p> <pre><code>from flask import Flask, url_for, request, json\n</code></pre> <p>Accessing the HTTP headers is done using the <code>request.headers</code> dictionary (\"dictionary-like object\") and the request data using the <code>request.data</code> string.</p> <p>A second nice module is Flask Restful. We can declare Resource class and use the API to link the resource to an URL.</p> <p>The following code illustrates the resource class, with an argument passed at the constructor level to inject it into the resource. In this case this is a Kafka consumer which includes a map of the message read. The class is using the Blueprint module to simplify the management of resource:</p> <pre><code># code of the resource.py\nfrom flask_restful import Resource, Api\nfrom flask import Blueprint\n\ndata_inventory_blueprint = Blueprint(\"data_inventory\", __name__)\ninventoryApi = Api(data_inventory_blueprint)\n\nclass DataInventory(Resource):  \n\n    def __init__(self, consumer):\n        self.consumer = consumer\n\n    # Returns the Inventory data in JSON format\n    @track_requests\n    @swag_from('data_inventory.yml')\n    def get(self):\n        logging.debug('[DataInventoryResource] - calling /api/v1/data/inventory endpoint')\n        return self.consumer.getAllLotInventory(),200, {'Content-Type' : 'application/json'}\n</code></pre> <p>The <code>app.py</code> that uses this resource, accesses the API and add_resource method, to define the resource class, the URL and then ant arguments to pass to the resource constructor.</p> <pre><code>from server.api.inventoryResource import data_inventory_blueprint, inventoryApi, DataInventory\n\n\napp = Flask(__name__)\n\ninventory_consumer = InventoryConsumer()\ninventoryApi.add_resource(DataInventory, \"/api/v1/data/inventory\",resource_class_kwargs={'consumer':inventory_consumer})\n</code></pre> <p>Flask REST API article</p>"},{"location":"flask/readme/#an-angular-app","title":"An Angular app","text":"<p>See this repository to a more complete example of angular development and Flask.</p>"},{"location":"flask/readme/#flask-tdd-docker","title":"Flask TDD Docker","text":"<p>See this dedicated note</p>"},{"location":"flask/readme/#flask-with-dynamodb-ecr-fargate","title":"Flask with DynamoDB, ECR, Fargate","text":"<p>See Code and Readme</p>"},{"location":"flask/readme/#flask-blueprint","title":"Flask Blueprint","text":"<p>Helps to structure the application in reusable components. To use in any Flask Blueprint, you have to import it and then register it in the application using <code>register_blueprint()</code>. A blueprint is an object that works like a flask app too. See the boiler plate example.</p>"},{"location":"python/faq/","title":"Python FAQ","text":""},{"location":"python/faq/#why-pipenv","title":"Why pipenv?","text":"<p>pipenv resolves the problem of dependencies management, that is not perfectly done in the requirements.txt, which leads to under deterministic build process. Given the same input (the requirements.txt file), pip doesn\u2019t always produce the same environment. <code>pip freeze</code> helps to freeze the dependencies and update the  requirements.txt. But any dependency change needs to be done manually, and you need to track the dependent package version, for bug fix, or mandatory security fixes.</p> <p>A second problem is the system wide repository used by pip. When developing multiple different projects in parallele that could become a real issue. <code>pipenv</code> use a per project environment. pipenv acts as pip + virtual environment. It uses Pipfile to replace requirements.txt and pipfile.lock for determnistic build. See this guide for command examples.</p>"},{"location":"python/faq/#how-to-get-program-arguments","title":"How to get program arguments?","text":"<p>See getopt</p> <pre><code>  import sys,getopt\n  USER=\"jbcodeforce\"\n  FILE=\"./data/export-questions.json\"\n  try:\n    opts, args = getopt.getopt(sys.argv,\"hi:u:\",[\"inputfile=\",\"user=\"])\n  except getopt.GetoptError:\n    print(usage())\n    sys.exit(2)\n\n\n  for opt, arg in opts:\n    if opt == '-h':\n      usage()\n      sys.exit()\n    elif opt in (\"-u\", \"--user\"):\n      USER = arg\n    elif opt in (\"-i\", \"--inputfile\"):\n      FILE = arg\n</code></pre>"},{"location":"python/faq/#using-arg_parser","title":"Using arg_parser","text":"<pre><code>import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"filename\", help=\"Must specify a file name\")\nparser.add_argument(\"--append\", help=\"Append records to existing file\",action=\"store_true\")\nargs = parser.parse_args()\nif args.append:\n    print(\"append to file\")\n</code></pre>"},{"location":"python/faq/#pass-a-variable-number-of-arguments-to-a-function","title":"Pass a variable number of arguments to a function","text":"<p>We can pass a variable number of arguments to a function using special symbols:</p> <ul> <li>*args (Non-Keyword Arguments): take in more arguments than the number of formal arguments that you previously defined</li> <li>**kwargs (Keyword Arguments):  used to pass a keyworded, variable-length argument list.</li> </ul> <pre><code>def myFun(**kwargs):\n    for key, value in kwargs.items():\n        print(\"%s == %s\" % (key, value))\n</code></pre>"},{"location":"python/faq/#what-__init__py-under-folder-used-for","title":"what <code>__init__.py</code> under folder used for?","text":"<p>The <code>__init__.py</code> file makes Python treat directories containing it as modules. Furthermore, this is the first file to be loaded in a module, so you can use it to execute code that you want to run each time a module is loaded, or specify the submodules to be exported.</p>"},{"location":"python/faq/#how-to-get-program-dependencies-generated","title":"How to get program dependencies generated?","text":"<p>To be able to run the python program at any time in the future, it is recommended to freeze the dependencies, so the requirements.txt will have the compatible module versions. </p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"python/faq/#develop-a-module-to-be-installable-with-pip","title":"Develop a module to be installable with pip","text":"<p>See this article, and Python Packaging User Guide summarized as:</p> <ol> <li>Create a python package (folder) with the setup.py in it. </li> </ol> <pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name=\"jb_module\",\n    version=\"0.1.0\",\n    description=\"The backend to support hybrid AI\",\n    author=\"Jerome Boyer\",\n    packages=find_packages(include=[\"acme\"]),\n    install_requires=['uvicorn', 'fastapi', 'langchain-openai','langchain-anthropic','langchain_ibm','langchain_community',\n'pydantic','python-multipart','python-dotenv','markdown','chromadb','pypdf'], \n)\n</code></pre> <ol> <li><code>pip3 install setuptool</code></li> <li>install in current virtual env: <code>pip install .</code> or <code>pip install --upgrade .</code>. Install with <code>pip uninstall jb_module</code></li> <li>Add a <code>__init__.py</code> file under the jb_module folder specify version...</li> </ol> <p><pre><code>__version__ = \"0.1.0\"\n__author__ = 'Jerome Boyer'\n__credits__ = ''\n</code></pre> 1. Create a source distribution with: <code>python setup.py sdist</code>, it contains a compressed archive of the package 1. Install twine to be able to upload to PyPi: <code>pip install twine</code> 1. Push the package to test.pypi.org</p>"},{"location":"python/faq/#access-environment-variable","title":"Access environment variable","text":"<p>Define environment variables in a <code>.env</code> file, use os package:</p> <pre><code>import os\n\nAWS_ACCESS_KEY_ID=os.environ.get(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n</code></pre> How to assess the type of an object? <p>'''python type(x) == str '''</p>"},{"location":"python/faq/#list-content-of-folder","title":"List content of folder","text":"<pre><code>import glob\n\ndef listOfYaml():\n    return glob.glob(\"./*.yaml\")\n</code></pre>"},{"location":"python/faq/#change-content-of-yaml","title":"Change content of yaml","text":"<pre><code>import glob\nimport yaml\n\ndef listOfYaml():\n    return glob.glob(\"./*.yaml\")\n\ndef processYamlFile(f):\n    with open(f) as aYaml:\n        listDoc = yaml.safe_load(aYaml)\n    print(listDoc)\n    listDoc[\"metadata\"][\"namespace\"]='std-2'\n    print(listDoc)\n\n\nf = listOfYaml()\nprocessYamlFile(f[0])\n</code></pre>"},{"location":"python/faq/#how-to-sort-unit-tests","title":"How to sort unit tests?","text":"<p>Use <code>TestSuite</code> and <code>TestRunner</code>. See TestPerceptron.py for usage.</p> <pre><code>import unittest\n\nclass TestPerceptron(unittest.TestCase):\n  # ....\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(TestPerceptron('testLoadIrisData'))\n    suite.addTest(TestPerceptron('testPlotIrisData'))\n\nif __name__ == \"__main__\":\n    runner = unittest.TextTestRunner(failfast=True)\n    runner.run(suite())\n</code></pre>"},{"location":"python/faq/#how-to-traverse-a-directory-hierarchy","title":"How to traverse a directory hierarchy?","text":"<pre><code>import osfor\nroot, dirs, files in os.walk(\"/mydir\"):    \n  for file in files:        \n    if file.endswith(\".txt\"):             \n      print(os.path.join(root, file))\n</code></pre>"},{"location":"python/faq/#how-to-select-a-random-item-from-a-list","title":"How to select a random item from a list?","text":"<pre><code>import random\nmove=random.choice(possibleMoves)\n</code></pre>"},{"location":"python/faq/#logger","title":"Logger","text":"<pre><code>import logging\nLOGGER = logging.getLogger(__name__)\nLOGGER.info(\"an interesting string\")\n</code></pre> <p>Start python with the <code>--log=INFO</code> to set the logging level.</p>"},{"location":"python/faq/#how-to-get-localization-in-python-message","title":"How to get localization in python message","text":"<p>How to make a string translated in another language using GNU gettext.</p> <p>See the code test_localize.py</p> <ul> <li>install poedit</li> <li>upadte PATH to point to <code>C:\\Program Files (x86)\\Poedit\\GettextTools\\bin</code></li> <li>get the localizable string from the python program</li> <li>xgettext -d app -o localize/app.pot test_localize.py</li> <li>then create localize/fr/LC_MESSAGES/app.po from the created app.po under localize</li> <li>Compile the po to mo: msgfmt -o localize/fr/LC_MESSAGES/app.mo localize/fr/LC_MESSAGES/app</li> <li>export LANG=fr</li> <li>python test_localize.py</li> </ul>"},{"location":"python/faq/#reading-files","title":"Reading Files","text":""},{"location":"python/faq/#read-json-file","title":"Read json file","text":"<pre><code>g = open('critics.json','r')\nd = json.load(g)\n</code></pre>"},{"location":"python/faq/#read-csv-file","title":"Read csv file","text":"<pre><code>f = open('fn.csv','r')\nfor line in f:\n  record = line.split(',')\n\n# or with unicode:\n  changedLine=u''.join(line).encode('utf-8').strip()\n</code></pre>"},{"location":"python/faq/#read-file-with-specific-encoding","title":"Read file with specific encoding","text":"<pre><code> with open('../data/movielens/u.item',  encoding='ISO-8859-1') as f:\n</code></pre>"},{"location":"python/faq/#skip-the-first-row-of-a-file","title":"Skip the first row of a file","text":"<pre><code>f = open('fn.csv','r')\nf.readline()\nfor line in f:\n</code></pre>"},{"location":"python/faq/#how-to-get-execution-time","title":"How to get execution time","text":"<pre><code>import time\nstart = time.perf_counter()\n# potentially slow computation\nend = time.perf_counter() - start\n</code></pre>"},{"location":"python/faq/#example-of-memory-consumption-for-object","title":"Example of memory consumption for object","text":"<pre><code>import sys\n\na = 3\nb = 3.123\nc = [a, b]\nd = []\nfor obj in [a, b, c, d]:\n  print(obj, sys.getsizeof(obj))\n</code></pre>"},{"location":"python/faq/#using-cloudevent","title":"Using CloudEvent","text":"<pre><code>attributes = {\n      \"type\": \"com.anycompany.bdcp.user\",\n      \"source\": \"https://anycompany.com/user-mgr\",\n}\ndata = { \"eventType\": \"UserLogin\", \"username\": \"bob.the.builder@superemail.com\"}\nevent = CloudEvent(attributes, data)\nprint(event)\n</code></pre>"},{"location":"python/faq/#what-is-zip","title":"What is zip?","text":"<p>Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterable:</p> <pre><code>dataset\n[[1, 20, 0], [2, 21, 1], [3, 22, 0]]\nfor a in zip(*dataset): print(a)\n(1, 2, 3)\n(20, 21, 22)\n(0, 1, 0)\n</code></pre>"},{"location":"python/faq/#how-to-use-some-math","title":"How to use some math","text":"<pre><code># modulo\n8 % 2\n0\n#\n</code></pre>"},{"location":"python/faq/#what-is-a-package","title":"What is a package?","text":"<p>A package is nothing more than a folder, which must contain a special file, init.py. (not needed anymore with python3.3+)</p>"},{"location":"python/faq/#what-are-namespace-and-scope","title":"What are namespace and scope?","text":"<p>A namespace is a mapping from names to objects. They are the built-in names, the global names in a module, and the local names in a function. A scope is a textual region of a Python program, where a namespace is directly accessible. There are four different scopes that Python makes accessible:</p> <ul> <li>The local scope, which is the innermost one and contains the local names.</li> <li>The enclosing scope, that is, the scope of any enclosing function. It contains non-local names and also non-global names.</li> <li>The global scope contains the global names.</li> <li>The built-in scope contains the built-in names.</li> </ul>"},{"location":"python/faq/#customize-matplotlib-graph","title":"customize matplotlib graph","text":"<pre><code>    graph.set_title(\"Results of 500 slot machine pulls\")\n    # Make the y-axis begin at 0\n    graph.set_ylim(bottom=0)\n    # Label the y-axis\n    graph.set_ylabel(\"Balance\")\n    # Bonus: format the numbers on the y-axis as dollar amounts\n    # An array of the values displayed on the y-axis (150, 175, 200, etc.)\n    ticks = graph.get_yticks()\n    # Format those values into strings beginning with dollar sign\n    new_labels = ['${}'.format(int(amt)) for amt in ticks]\n    # Set the new labels\n    graph.set_yticklabels(new_labels)\n</code></pre>"},{"location":"python/faq/#how-to-program-web-socket-server","title":"How to program web socket server?","text":"<p>See FastAPI doc with testing and matching code in websocket_server</p>"},{"location":"python/faq/#using-async-io","title":"Using async IO","text":"<p>Multiprocessing is ideal for CPU-bound tasks (for-loop code) and uses multiple core to run code in parallel. Concurrency is the property to run in an overlapping manner. Threading is a concurrent execution model whereby multiple threads take turns executing tasks. Threads are used for IO-bound jobs (waiting on input/output to complete). The Python aAsync IO is a single-threaded, single-process design: it uses cooperative multitasking. See this tutorial: Async io is supported by <code>async</code> and <code>await</code> language keywords. Asynchronous routines are able to pause while waiting on their ultimate result and let other routines run in the meantime. async model is built around concepts such as callbacks, events, transports, protocols, and futures. <code>await</code> passes the function control back to the event loop. A coroutine is a function that can suspend its execution before reaching return. The <code>async def</code> for a function, creates a native coroutine, or an asynchronous generator. See code in api_stream.   <code>async for</code> and <code>async with</code> are generators too. To call a coroutine function, you must await it to get its results. <code>test_stream()</code> is suspended until it got the results from <code>call_llm_astream()</code>:</p> <pre><code>async def test_stream():\n  await call_llm_astream()\n</code></pre> <p>To run a corouting, we need the python asyncio module:</p> <pre><code>async def main():\n  await call_to_an_async_function()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>yield keyword in an <code>async def</code> block creates an asynchronous generator, which you iterate over with <code>async for</code>. A generator defined await and next methods. So it pushes value to calling stack at the yield level, but it also keeps a hold of its local variables when the program resume it by calling next() on it (<code>async for</code> calls the next() implicitly as it is a asynchronous iterator). </p> <p>By default, an async IO event loop runs in a single thread and on a single CPU core.</p>"},{"location":"python/pydantic/","title":"Pydantic","text":"<p>Pydantic is a data validation library for Python, schema validation and serialization are controlled by type annotations.</p> <p>Interesting constructs:</p> <ul> <li>BaseSettings Base class for settings, allowing values to be overridden by environment variables. Attention import pydantic_settings and install pydantic-settings</li> <li>YamlConfigSettingsSource get configuration from Yaml file</li> </ul>"},{"location":"python/python-summary/","title":"Python Summary","text":""},{"location":"python/python-summary/#concepts","title":"Concepts","text":""},{"location":"python/python-summary/#datatypes","title":"Datatypes","text":"<ul> <li> <p><code>list</code>:</p> <ul> <li>concat lists: a = [1,2,3,4]    then a = a + [5,6]  or a + list(\"789\") -&gt; [1,2,3,4,5,6,'7','8','9']. Lists are mutable.</li> <li>len(a)</li> <li>slicing: all elements except first and last: <code>a[1:-1]</code>, all from index 3: <code>a[3:]</code> </li> <li><code>list.append(a_record)</code> modifies a list by adding an item to the end</li> <li><code>list.pop()</code> removes and returns the last element of a list</li> <li>Get one object index using the <code>list.index(object)</code> </li> <li><code>in list</code> to assess if element in the list</li> <li> <p>list comprehensions:</p> </li> <li> <p><code>squares = [n**2 for n in range(10)]</code></p> </li> <li><code>short_planets = [planet for planet in planets if len(planet) &lt; 6]</code></li> </ul> </li> <li> <p><code>dictionary</code> is like json object, with key-value list. The main operations on a dictionary are storing a value with some key and extracting the value given the key. It is also possible to delete a key:value pair with del. If you store using a key that is already in use, the old value associated with that key is forgotten.</p> </li> </ul> <pre><code>cols={}\ncols(\"column_name_1\") = np.random.normal(2,1,10)\n</code></pre> <ul> <li>A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. <code>squares = [x**2 for x in range(10)]</code></li> <li>Queues: do not use list for queue but collections.deque</li> </ul> <pre><code>from collections import deque\n    queue = deque([23,56,78,44])\n    queue.append(55)\nprint(queue)\n\n&gt; deque([23, 56, 78, 44, 55])\ntwentythree=queue.popleft()\n</code></pre>"},{"location":"python/python-summary/#tuples","title":"Tuples","text":"<p>tuples: a = (1,2,3,4,5) are iterable. Tuple does not support item assignment: t[3] = 5 \u2192 error.</p> <pre><code>tup1 = ('physics', 'chemistry', 1997, 2000);\nprint (\"tup1[0]: \", tup1[0]);\n# iteration\nfor a in tup1:\n  print(a)\n</code></pre> <p>They are immutable. Need to create new tuples from existing one. Removing individual tuple elements is not possible.</p> <p>Transform a tuple into array: <pre><code>a=(2, 2.6496666666666666, -30.463416666666667)\nb=np.asarray(a)\n# b array([  2.        ,   2.64966667, -30.46341667])\n</code></pre></p>"},{"location":"python/python-summary/#string","title":"String","text":"<pre><code># get the last four chars\ndirname[:len(dirname) - 4]\n# split string to get the last folder name of a path\nfolders = path.split('/') \nname= folders[len(folders)-1]\nname.lower()\nname.upper()\nname.index('substr')\nclaim.startswith(planet)\nclaim.endswith(planet)\n</code></pre> <p>See string tutorial</p> <pre><code>def word_search(documents, keyword):\n    \"\"\"\n    Takes a list of documents (each document is a string) and a keyword. \n    Returns list of the index values into the original list for all documents \n    containing the keyword.\n\n    Example:\n    doc_list = [\"The Learn Python Challenge Casino.\", \"They bought a car\", \"Casinoville\"]\n    &gt;&gt;&gt; word_search(doc_list, 'casino')\n    &gt;&gt;&gt; [0]\n    \"\"\"\n    # list to hold the indices of matching documents\n    indices = [] \n    # Iterate through the indices (i) and elements (doc) of documents\n    for i, doc in enumerate(documents):\n        # Split the string doc into a list of words (according to whitespace)\n        tokens = doc.split()\n        # Make a transformed list where we 'normalize' each word to facilitate matching.\n        # Periods and commas are removed from the end of each word, and it's set to all lowercase.\n        normalized = [token.rstrip('.,').lower() for token in tokens]\n        # Is there a match? If so, update the list of matching indices.\n        if keyword.lower() in normalized:\n            indices.append(i)\n    return indices\n</code></pre>"},{"location":"python/python-summary/#control-flow","title":"Control flow","text":"<p><code>if condition: elsif condition: else</code></p> <ul> <li>For statement iterates over the items of any sequence (a list or a string), in the order that they appear in the sequence</li> <li>Range() to iterate over a sequence of numbers. (e.g. for i in range(5):). The given end point is never part of the generated sequence. It is possible to let the range start at another number, or to specify a different increment (from 0 to 10, increment 3:   range(0, 10, 3)). It returns an object which returns the successive items of the desired sequence when you iterate over it</li> <li>list(range(5)) build a list like: [0, 1, 2, 3, 4]</li> <li>Loop statements may have an else clause; it is executed when the loop terminates through exhaustion of the list (with for) or when the condition becomes false (with while), but not when the loop is terminated by a break statement</li> <li> <p>The pass statement does nothing. It can be used when a statement is required syntactically but the program requires no action.</p> </li> <li> <p>See Control Flow Statement Tutorial</p> </li> </ul>"},{"location":"python/python-summary/#exception","title":"Exception","text":"<pre><code>try:\n  dosomething()\nExcept ValueError:\n  pass\n</code></pre>"},{"location":"python/python-summary/#regular-expressions","title":"Regular Expressions","text":"<p>How to regex</p> <p>Specialize in string pattern matching from string. It is a language by itself.</p> <pre><code>import re\np = re.compile('ab*')\n</code></pre> Char Note ^ Matches the beginning of a line $ Matches the end of the line . Matches any character \\s Matches whitespace \\S Matches any non-whitespace character * Repeats a character zero or more times *? Repeats a character zero or more times (non-greedy) + Repeats a character one or more times +? Repeats a character one or more times (non-greedy) [aeiou] Matches a single character in the listed set [^XYZ] Matches a single character not in the listed set [a-z0-9] The set of characters can include a range ( Indicates where string extraction is to start ) Indicates where string extraction is to end \u2018@([^ ]*)' extract the domain name from the email. Use () to specify what to extract, from the @. [^ ] math non-blank character re.findall(\u2018[0-9]+\u2019,s) find all occurrence of number in string. [0-9] is one digit"},{"location":"python/python-summary/#functions","title":"Functions","text":"<p>Python supports OOD and functional programming like Scala. Function can be defined in a scope of a module file outside of a class, or as method of a class.</p> <ul> <li>The keyword <code>def</code> introduces a function definition. It must be followed by the function name and the parenthesized list of formal parameters. The statements that form the body of the function start at the next line, and must be indented.</li> <li>The first statement of the function body can optionally be a string literal used as <code>docstring</code>.</li> <li>local variables cannot be directly assigned a value within a function (unless named in a global statement), although they may be referenced</li> <li>The return statement returns with a value from a function. return without an expression argument returns <code>None</code>.</li> <li>functions can have a variable number of arguments <code>def ask_ok(prompt, retries=4, complaint='Yes or no, please!'):</code></li> <li>Functions can also be called using keyword arguments of the form <code>kwarg=value</code> instead of using the positional arguments.  keyword arguments must follow positional arguments. Arguments could have default value so becomes optional. Attention the default value is evaluated only once. This makes a difference when the default is a mutable object such as a list, dictionary, or instances of most classes. For example, the following function accumulates the arguments passed to it on subsequent calls</li> </ul> <pre><code>def f(a, L=[]):\n    L.append(a)\n    return L\n\nprint(f(1))\n[1]\nprint(f(2))\n[1,2]\nprint(f(3))\n[1,2,3]\n</code></pre> <ul> <li> <p>A function can be called with an arbitrary number of arguments. The syntax is <code>def (formalArg,formalArg2,*args,kwarg=value):</code></p> </li> <li> <p>lambda is a keyword to define an anonymous function</p> </li> </ul> <p><pre><code>def make_incrementor(n):\n      return lambda x: x + n\nf = make_incrementor(42)\nf(2)\n44\n</code></pre> This can be used for specifying a sort method anonymously: use the second element of a tuple to sort a list of tuples</p> <pre><code>collection.sort(key=lambda collection : collection[1])\n</code></pre> <p>variable scope in function: when there is a need to access variable defined at the module level use the keyword global</p>"},{"location":"python/python-summary/#namespaces","title":"Namespaces","text":"<p>A namespace is a mapping from names to objects. Examples of namespaces are:</p> <ul> <li>the set of built-in names (containing functions such as abs(), and built-in exception names), loaded when interpreter starts</li> <li>the global names in a module, created when module is loaded, and kept until interpreter quits</li> <li>the local names in a function invocation, created when the functions is called, deleted when function returns or raises an exception. When searching of reference the interpreter starts by the innermost scope (current block) then enclosing functions, modules and built-in names It is important to realize that scopes are determined textually: the global scope of a function defined in a module is that module\u2019s namespace, no matter from where or by what alias the function is called. If no global statement is in effect \u2013 assignments to names always go into the innermost scope. Assignments do not copy data \u2014 they just bind names to objects.</li> </ul> <pre><code># reference a non local variable in a function\n     nonlocal spam\n# or a global\n    global spam\n</code></pre>"},{"location":"python/python-summary/#object","title":"Object","text":"<p>Python is a OOP, with polymorphism and inheritance.</p> <p>A class construction is a method declare as <code>def __init__(self):</code> A destructor is <code>def __del__(self):</code>. A toString is <code>def _str__():</code></p> <p>Class definitions, like function definitions (def statements) must be executed before they have any effect. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new 'class' namespace.</p> <pre><code>class Complex:\n    \"\"\" Represents mathematical Complex number\n    \"\"\"\n    # called once instance is created to initialize internal attributes (like java constructor)\n    def __init__(self, realpart, imagpart):\n        self.r = realpart\n        self.i = imagpart\n\n\n# creating an instance\nx=Complex(3,2)\n# attribute reference ; class.attname\nx.i\n</code></pre> <p>The only operations understood by instance objects are attribute references. There are two kinds of valid attribute names, data attributes and methods.</p> <pre><code>class MyClass(object):\n    '''\n    classdocs\n    '''\n\n    def __init__(self, p:str):\n        '''\n        Constructor\n        '''\n        self.a=p\n        self.n=1\n    def f(self):\n        return self.n+1\n\nc=MyClass('an object')\n# can add attribute dynamically into object even if class did not define it...\nc.b=\"a new attribute\"\nprint(c.a)\n# an object\nprint(c.b)\n# a new attribute\nprint(c.f())\n# 2\n</code></pre> <p>Clients should use data attributes with care \u2014 clients may mess up invariants maintained by the methods by stamping on their data attributes. Python supports inheritance and search for attributes is done using a depth-first, left to right approach.</p> <pre><code>class DerivedClassName(Base1, Base2, Base3):\n</code></pre> <p>There is no private instance variables inside an object. The naming convention using _ before the name should be treated as non-public part of the API.</p>"},{"location":"python/python-summary/#module","title":"Module","text":"<p>A module is a file containing python definitions and statements. The filename = module name.  Definitions from a module can be imported into other modules or into the main module.  Be sure to take the folder hierarchy as package hierarchy. A module can contain executable statements as well as function definitions. These statements are  intended to initialize the module. They are executed only the first time the module name is  encountered in an import statement.</p> <p><pre><code># To see the variables and function of a module\nimport math\nprint(dir(math))\n# give combined documentation for all the functions and values in the module \nhelp(math)\n</code></pre> Always import only the specific things we'll need from each module.</p> <p>To make a module executable we need a main statement</p> <pre><code>if __name__ == \"__main__\":\n</code></pre> <p>The directory containing the script being run is placed at the beginning of the search path, ahead of the standard library path: scripts in that directory will be loaded instead of modules of the same name in the library directory. The interpreter first searches for a built-in module then it searches for a file named spam.py in a list of directories given by the variable sys.path (current directory + PYTHONPATH). To speed up loading modules, Python caches the compiled version of each module in the pycache directory under the name module.version.pyc, where the version encodes the format of the compiled file; it generally contains the Python version number.</p>"},{"location":"python/python-summary/#file-io","title":"File I/O","text":"<p>You open a file in different mode with the open . Files are text or binary <pre><code># first create a text file\nf = open('atext .txt','w' )\nf.write('A first line\\n')\nf.write('A second line\\n')\nf.close()\n\nf = open('atext.txt', 'r')\nf.readline()\n# 'A first line\\n'\nf.readline()\n# 'A second line\\n'\nf.readline()\n# no more line is '' empty string\n\n# read all lines and build a list: 2 ways\nlines=f.readlines()\nlist(f)\n# read line by line: very efficient as use limited memory. f is an iterator over the lines\nfor line in f\n\n# a dictionary persisted as json in text file\nimport json\nf = open('critics.txt', 'w')\njson.dump(critics,f)\nf.close()\n# reload it\ng = open( 'critics.txt','r' )\nd=json.load(g)\nprint(d[' Toby'])\n</code></pre></p> <p>Python doesn't flush the buffer\u2014that is, write data to the file\u2014until it's sure you're done writing. One way to do this is to close the file. File objects contain a special pair of built-in methods: <code>__enter__()</code> and <code>__exit__()</code>.</p> <p>See code <code>python-bible/readAssetFromFolder.py</code> which uses Git client to get origin URL.</p>"},{"location":"python/python-summary/#date","title":"Date","text":"<p>See the datetime module</p> <pre><code>import datetime\n\nprint ('Current date/time: {}'.format(datetime.datetime.now()))\n\n d= datetime.date(2018,9,23)\n d= datetime.date.today()\n datetime.datetime.today()\n\ndatetime.datetime(2019, 9, 23, 18, 34, 26, 856722)\n\n\ndate_time_str = 'Jun 28 2018  7:40AM'\ndate_time_obj = datetime.datetime.strptime(date_time_str, '%b %d %Y %I:%M%p')\n\n# transform to a string\nd.strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"python/python-summary/#unit-testing","title":"Unit testing","text":"<p>unittest is based on Kent Beck's work on unit testing like the <code>junit</code> library.</p> <ul> <li> <p>define a module with a class which extends TestCase, use the setUp and tearDown methods to set context before each test method.</p> </li> <li> <p>Add test method and use assert* to validate test results.</p> </li> </ul> <p>Consider pytest as another modern tool to do testing in python. </p>"},{"location":"python/python-summary/#reading-command-line-arguments","title":"Reading command line arguments","text":"<pre><code>import sys\nprint(\"This is the name of the script: \", sys.argv[0])\nprint(\"Number of arguments: \", len(sys.argv))\nprint(\"The arguments are: \" , str(sys.argv))\n</code></pre>"},{"location":"python/python-summary/#doing-http-requests","title":"Doing HTTP requests","text":"<p>See code under web_data, but with python 3 the approach is to use request.</p> <ul> <li>urllib</li> <li>The request library</li> </ul>"},{"location":"python/python-summary/#python-flask-webapp","title":"Python Flask WebApp","text":"<p>The project python-code includes the <code>angular-flask</code> folder to present some simple examples of how to use Flask with Angular. </p> <p>See this note for details.</p>"},{"location":"python/python-summary/#data-management","title":"Data management","text":""},{"location":"python/python-summary/#pandas","title":"Pandas","text":"<p>Create a data frame with two columns</p> <pre><code>data = DataFrame({'message': [], 'class': []})\n</code></pre> <p>Create n records with timestamp from one start time:</p> <pre><code>start_time = datetime.datetime.today() \nc=pd.date_range(start_time, periods=nb_records, freq=METRIC_FREQUENCY)\n</code></pre> <p>Transforming to string <pre><code>c.strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre></p>"},{"location":"python/python-summary/#split-data-into-training-and-test-sets","title":"Split data into training and test sets","text":"<pre><code>splitIndex = np.random.rand(len(data)) &lt; 0.8\ntrain = data[splitIndex]\ntest = data [~splitIndex]\n</code></pre>"},{"location":"techno/aws/","title":"AWS","text":""},{"location":"techno/aws/#boto3-library","title":"boto3 library","text":"<p>A unique library to access all AWS services from a python app. </p>"},{"location":"techno/aws/#installation","title":"Installation","text":"<pre><code>pip install boto3[crt]\n</code></pre> <p>Set up authentication credentials for your AWS account using either the IAM Console or the AWS CLI.</p> <pre><code>aws configure\n# Verify access\naws iam list-users\n</code></pre> <p>Info</p> <p>The jbcodeforce/python docker image has the aws cli and goto3.</p>"},{"location":"techno/aws/#programming-samples","title":"Programming samples","text":""},{"location":"techno/aws/#access-s3","title":"Access S3","text":"<pre><code>import boto3\n# declare a client to the service you want\ns3 = goto3.service(\"s3\")\n# use SDK API for s3.\ns3.buckets.all()\n</code></pre>"},{"location":"techno/aws/#access-dynamodb","title":"Access DynamoDB","text":"<p>The client can get the table name using the API client: </p> <pre><code>import os, boto3\n\nAWS_ACCESS_KEY_ID=os.environ.get(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n\nclient = boto3.client(\n    'dynamodb',\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    )\ntable = client.list_tables()\ntableName=table['TableNames'][0]\n</code></pre> <p>Then use the dynamoDB API:</p> <pre><code>orderTable = dynamodb.Table(tableName)\n\ndynamodb = boto3.resource(\n    'dynamodb',\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    )\n\norderTable = dynamodb.Table(tableName)\norderTable.put_item(\n   Item={\n     \"orderID\": \"ORD001\",\n     \"customerID\": \"C01\", \n     \"productID\": \"P01\", \n     \"quantity\": 10,  \n     \"destinationAddress\": { \"street\": \"1st main street\", \"city\": \"Santa Clara\", \"country\": \"USA\", \"state\": \"CA\", \"zipcode\": \"95051\" }\n   })\n</code></pre> <ul> <li>Run it once the python virtual env is enabled with <code>python dynamoClient.py</code></li> <li>scan to run all items from a table. It performs eventually consistent reads</li> <li>put_item: If an item that has the same primary key as the new item already exists in the specified table, the new item completely replaces the existing item</li> </ul>"},{"location":"techno/aws/#cdk-with-python","title":"CDK with python","text":"<p>Separate note in AWS_Studies.</p>"}]}